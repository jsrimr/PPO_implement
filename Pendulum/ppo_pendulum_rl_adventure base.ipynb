{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#관전포인트 : 경험모을 때 gpu 를 사용할 수 있는가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.permutation(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ids = np.split(ids[:batch_size // mini_batch_size * mini_batch_size], batch_size // mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda:1\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "#         self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage): # 전체 데이터 셋에서 mini_batch 를 만드는 것이다.\n",
    "    batch_size = states.size(0)\n",
    "    ids = np.random.permutation(batch_size)\n",
    "    ids = np.split(ids[:batch_size // mini_batch_size * mini_batch_size], batch_size // mini_batch_size)\n",
    "    for i in range(len(ids)):\n",
    "        yield states[ids[i], :], actions[ids[i], :], log_probs[ids[i], :], returns[ids[i], :], advantage[ids[i], :]\n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2): # training\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 32\n",
    "lr               = 1e-3\n",
    "num_steps        = 128\n",
    "mini_batch_size  = 256\n",
    "ppo_epochs       = 30\n",
    "threshold_reward = -100\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 15000\n",
    "frame_idx  = 0\n",
    "test_rewards = []\n",
    "early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states 128\n",
      "state_shape (16, 3)\n",
      "state [[ 0.84686884 -0.53180181 -3.72222422]\n",
      " [-0.5837327  -0.81194589 -5.51436465]\n",
      " [ 0.97199038  0.23502065 -4.74187172]\n",
      " [ 0.9878556  -0.15537472 -4.8302156 ]\n",
      " [ 0.39222805 -0.91986801 -3.81595192]\n",
      " [ 0.07569987  0.99713065 -6.67020438]\n",
      " [ 0.59709577 -0.80216996 -5.37162599]\n",
      " [-0.49253922 -0.87029025 -7.59325613]\n",
      " [-0.87450187  0.48502214  0.90836877]\n",
      " [ 0.85135621 -0.52458803 -4.65209622]\n",
      " [ 0.85719893  0.51498544 -4.80911915]\n",
      " [-0.25246806  0.96760523 -3.58420635]\n",
      " [-0.68369031 -0.72977227 -1.50603343]\n",
      " [-0.09939207  0.99504835 -7.04099248]\n",
      " [ 0.09976505 -0.99501102  5.42113061]\n",
      " [-0.50714327  0.86186177 -7.72818957]]\n",
      "next_state: [[ 0.84686884 -0.53180181 -3.72222422]\n",
      " [-0.5837327  -0.81194589 -5.51436465]\n",
      " [ 0.97199038  0.23502065 -4.74187172]\n",
      " [ 0.9878556  -0.15537472 -4.8302156 ]\n",
      " [ 0.39222805 -0.91986801 -3.81595192]\n",
      " [ 0.07569987  0.99713065 -6.67020438]\n",
      " [ 0.59709577 -0.80216996 -5.37162599]\n",
      " [-0.49253922 -0.87029025 -7.59325613]\n",
      " [-0.87450187  0.48502214  0.90836877]\n",
      " [ 0.85135621 -0.52458803 -4.65209622]\n",
      " [ 0.85719893  0.51498544 -4.80911915]\n",
      " [-0.25246806  0.96760523 -3.58420635]\n",
      " [-0.68369031 -0.72977227 -1.50603343]\n",
      " [-0.09939207  0.99504835 -7.04099248]\n",
      " [ 0.09976505 -0.99501102  5.42113061]\n",
      " [-0.50714327  0.86186177 -7.72818957]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "while frame_idx < 20000 and not early_stop: #15000 epoch 반복\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps): #경험 모으기 - gpu 쓰는구나 . 하지만 여전히 DataParallel 들어갈 여지는 없어보인다. \n",
    "        #-> 아.. state 가 벡터 1개가 아닐 것 같다.. 16개네. gpu 쓸만하다. DataParallel 도 가능할듯?\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy()) #done 한다고 끝내질 않네??\n",
    "        \n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 1000 == 0: # 1000번 마다 plot 그려주기\n",
    "            test_reward = np.mean([test_env() for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            torch.save(model.state_dict(),\"weight/updated_ppo_weight_{}.pth\".format(frame_idx))\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "\n",
    "    print(\"states\", len(states))\n",
    "    print(\"state_shape\" , state.shape)\n",
    "    print(\"state\" , state)\n",
    "    print(\"next_state:\", next_state)\n",
    "    break\n",
    "    #경험 1세트 모은거로 학습하기\n",
    "    \n",
    "    #num_step 만큼 진행했을 때 reward 얼마였는지 구하기\n",
    "    next_state = torch.FloatTensor(next_state).to(device) \n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving trajectories for GAIL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -6.005084751638878\n",
      "episode: 1 reward: -126.8927871226066\n",
      "episode: 2 reward: -133.36746402003962\n",
      "episode: 3 reward: -266.05918127231354\n",
      "episode: 4 reward: -339.21731628644164\n",
      "episode: 5 reward: -131.8278783064484\n",
      "episode: 6 reward: -248.87367431578727\n",
      "episode: 7 reward: -137.175333194117\n",
      "episode: 8 reward: -6.328519695053957\n",
      "episode: 9 reward: -137.1696418909525\n",
      "episode: 10 reward: -252.9144404954698\n",
      "episode: 11 reward: -129.00562978825454\n",
      "episode: 12 reward: -268.5682250542064\n",
      "episode: 13 reward: -395.79137086108\n",
      "episode: 14 reward: -254.1389557076782\n",
      "episode: 15 reward: -135.80559049099568\n",
      "episode: 16 reward: -136.58210836012515\n",
      "episode: 17 reward: -256.44810599440734\n",
      "episode: 18 reward: -260.0060756338123\n",
      "episode: 19 reward: -133.67263772828153\n",
      "episode: 20 reward: -129.79663430485184\n",
      "episode: 21 reward: -130.3333707007226\n",
      "episode: 22 reward: -265.65811345573445\n",
      "episode: 23 reward: -135.6062894944192\n",
      "episode: 24 reward: -264.2079099342339\n",
      "episode: 25 reward: -131.09634461279475\n",
      "episode: 26 reward: -135.64332463087422\n",
      "episode: 27 reward: -264.7361103369829\n",
      "episode: 28 reward: -132.28003020080362\n",
      "episode: 29 reward: -135.64667353873446\n",
      "episode: 30 reward: -257.48897455480477\n",
      "episode: 31 reward: -129.460450419052\n",
      "episode: 32 reward: -261.3846860643582\n",
      "episode: 33 reward: -133.06304114014375\n",
      "episode: 34 reward: -257.40383277117246\n",
      "episode: 35 reward: -262.86106253715695\n",
      "episode: 36 reward: -260.14711587728056\n",
      "episode: 37 reward: -8.122377740098576\n",
      "episode: 38 reward: -132.77976135163487\n",
      "episode: 39 reward: -131.7167280385313\n",
      "episode: 40 reward: -261.68291699212733\n",
      "episode: 41 reward: -254.26903327429198\n",
      "episode: 42 reward: -8.527744067384816\n",
      "episode: 43 reward: -244.82191202634067\n",
      "episode: 44 reward: -258.990782127511\n",
      "episode: 45 reward: -141.01938262247828\n",
      "episode: 46 reward: -263.42741141746137\n",
      "episode: 47 reward: -135.36822728946098\n",
      "episode: 48 reward: -5.980236579083558\n",
      "episode: 49 reward: -135.62510456860105\n",
      "episode: 50 reward: -131.7411989164515\n",
      "episode: 51 reward: -6.975778185567841\n",
      "episode: 52 reward: -267.85062200703413\n",
      "episode: 53 reward: -135.550367751227\n",
      "episode: 54 reward: -136.48922769142786\n",
      "episode: 55 reward: -255.61404629340672\n",
      "episode: 56 reward: -134.35864807964776\n",
      "episode: 57 reward: -398.6016116011461\n",
      "episode: 58 reward: -273.0615168461912\n",
      "episode: 59 reward: -138.34937950961694\n",
      "episode: 60 reward: -133.2936691411967\n",
      "episode: 61 reward: -132.57629824254553\n",
      "episode: 62 reward: -135.53870681569336\n",
      "episode: 63 reward: -133.67160759657665\n",
      "episode: 64 reward: -293.6449235079289\n",
      "episode: 65 reward: -243.88367082986167\n",
      "episode: 66 reward: -259.4207209315094\n",
      "episode: 67 reward: -9.868409142171531\n",
      "episode: 68 reward: -7.913167415113294\n",
      "episode: 69 reward: -276.50739889986744\n",
      "episode: 70 reward: -8.91212695847403\n",
      "episode: 71 reward: -5.583002133789065\n",
      "episode: 72 reward: -6.9757821433553655\n",
      "episode: 73 reward: -137.1022643397585\n",
      "episode: 74 reward: -139.55356557207122\n",
      "episode: 75 reward: -131.62757602025638\n",
      "episode: 76 reward: -138.58913421469376\n",
      "episode: 77 reward: -280.433356662738\n",
      "episode: 78 reward: -6.801709799563931\n",
      "episode: 79 reward: -285.9462694268035\n",
      "episode: 80 reward: -132.94257940845657\n",
      "episode: 81 reward: -645.1860092584074\n",
      "episode: 82 reward: -251.4640906231599\n",
      "episode: 83 reward: -132.31070908484205\n",
      "episode: 84 reward: -250.1329790745068\n",
      "episode: 85 reward: -132.86472940296952\n",
      "episode: 86 reward: -264.0642621435206\n",
      "episode: 87 reward: -407.4713790055894\n",
      "episode: 88 reward: -126.98414790152339\n",
      "episode: 89 reward: -257.19524825378255\n",
      "episode: 90 reward: -253.16972838317042\n",
      "episode: 91 reward: -243.92531278959368\n",
      "episode: 92 reward: -132.03010992741468\n",
      "episode: 93 reward: -270.675388315995\n",
      "episode: 94 reward: -504.71650252179217\n",
      "episode: 95 reward: -250.55141848416733\n",
      "episode: 96 reward: -6.926877372185537\n",
      "episode: 97 reward: -128.34538264085708\n",
      "episode: 98 reward: -133.98128336666576\n",
      "episode: 99 reward: -135.13052075345925\n",
      "episode: 100 reward: -129.78505500247047\n",
      "episode: 101 reward: -6.481930479095035\n",
      "episode: 102 reward: -136.07268485017636\n",
      "episode: 103 reward: -131.85806413272775\n",
      "episode: 104 reward: -136.198288143197\n",
      "episode: 105 reward: -133.3620512667381\n",
      "episode: 106 reward: -530.747330216091\n",
      "episode: 107 reward: -8.46529592849284\n",
      "episode: 108 reward: -260.3202935511769\n",
      "episode: 109 reward: -132.22017038605273\n",
      "episode: 110 reward: -137.50448711464222\n",
      "episode: 111 reward: -249.01656980431883\n",
      "episode: 112 reward: -128.59393869695177\n",
      "episode: 113 reward: -134.3081577656609\n",
      "episode: 114 reward: -128.39278525216065\n",
      "episode: 115 reward: -129.52001454212893\n",
      "episode: 116 reward: -136.969276014437\n",
      "episode: 117 reward: -588.343631487925\n",
      "episode: 118 reward: -407.39406384467514\n",
      "episode: 119 reward: -252.10417073896087\n",
      "episode: 120 reward: -127.04489135094825\n",
      "episode: 121 reward: -7.641384031784773\n",
      "episode: 122 reward: -128.81633945694605\n",
      "episode: 123 reward: -252.765915195525\n",
      "episode: 124 reward: -126.7709793161884\n",
      "episode: 125 reward: -129.6921019340072\n",
      "episode: 126 reward: -408.3035245286246\n",
      "episode: 127 reward: -244.77164969426752\n",
      "episode: 128 reward: -271.1434503409498\n",
      "episode: 129 reward: -10.15523641025113\n",
      "episode: 130 reward: -375.47744988141824\n",
      "episode: 131 reward: -133.8157514759128\n",
      "episode: 132 reward: -137.2784572550647\n",
      "episode: 133 reward: -408.0734302820434\n",
      "episode: 134 reward: -128.34852628645393\n",
      "episode: 135 reward: -256.7462958054539\n",
      "episode: 136 reward: -260.11460058620094\n",
      "episode: 137 reward: -353.99117919441574\n",
      "episode: 138 reward: -7.158197297421622\n",
      "episode: 139 reward: -395.8101207133651\n",
      "episode: 140 reward: -6.151456005902488\n",
      "episode: 141 reward: -426.3155351168951\n",
      "episode: 142 reward: -136.85707738113996\n",
      "episode: 143 reward: -6.893854802637803\n",
      "episode: 144 reward: -11.256213306850762\n",
      "episode: 145 reward: -135.3371172519894\n",
      "episode: 146 reward: -134.83140859900595\n",
      "episode: 147 reward: -133.5312638494464\n",
      "episode: 148 reward: -135.90636406681782\n",
      "episode: 149 reward: -250.35033696038843\n",
      "episode: 150 reward: -359.8853364491908\n",
      "episode: 151 reward: -259.1684138449847\n",
      "episode: 152 reward: -4.655777876815764\n",
      "episode: 153 reward: -268.6340689314077\n",
      "episode: 154 reward: -262.59212247841026\n",
      "episode: 155 reward: -128.9161938163539\n",
      "episode: 156 reward: -134.2113931945993\n",
      "episode: 157 reward: -134.97089406233323\n",
      "episode: 158 reward: -384.23207206403237\n",
      "episode: 159 reward: -135.65444435156064\n",
      "episode: 160 reward: -6.213192694264863\n",
      "episode: 161 reward: -7.905469724835222\n",
      "episode: 162 reward: -274.49551971605473\n",
      "episode: 163 reward: -133.68467910828684\n",
      "episode: 164 reward: -6.665339014920036\n",
      "episode: 165 reward: -133.72212928292237\n",
      "episode: 166 reward: -269.00924761742897\n",
      "episode: 167 reward: -131.2742993827836\n",
      "episode: 168 reward: -133.94458970843942\n",
      "episode: 169 reward: -374.22035710323104\n",
      "episode: 170 reward: -133.85976034942894\n",
      "episode: 171 reward: -266.0817743769959\n",
      "episode: 172 reward: -257.8740659582139\n",
      "episode: 173 reward: -281.16967484913863\n",
      "episode: 174 reward: -129.2436462659464\n",
      "episode: 175 reward: -251.00041620787988\n",
      "episode: 176 reward: -131.64720715168872\n",
      "episode: 177 reward: -272.5222440394314\n",
      "episode: 178 reward: -255.26667776435437\n",
      "episode: 179 reward: -135.04428000112873\n",
      "episode: 180 reward: -129.13459279300696\n",
      "episode: 181 reward: -308.88598229326067\n",
      "episode: 182 reward: -136.35354810892528\n",
      "episode: 183 reward: -11.734697564841575\n",
      "episode: 184 reward: -251.5529901338912\n",
      "episode: 185 reward: -6.9724005578129855\n",
      "episode: 186 reward: -136.70162646196485\n",
      "episode: 187 reward: -134.58019500731592\n",
      "episode: 188 reward: -246.9933926444815\n",
      "episode: 189 reward: -135.12816762358548\n",
      "episode: 190 reward: -137.72738811281144\n",
      "episode: 191 reward: -138.87651457944284\n",
      "episode: 192 reward: -134.97108090545325\n",
      "episode: 193 reward: -5.95376352303051\n",
      "episode: 194 reward: -134.5083328993307\n",
      "episode: 195 reward: -133.44839687462886\n",
      "episode: 196 reward: -421.1042197791855\n",
      "episode: 197 reward: -135.81672813526336\n",
      "episode: 198 reward: -133.69244062689643\n",
      "episode: 199 reward: -249.10207750535864\n",
      "episode: 200 reward: -131.66516289349124\n",
      "episode: 201 reward: -133.6244420246075\n",
      "episode: 202 reward: -397.83002037419686\n",
      "episode: 203 reward: -137.67627110251067\n",
      "episode: 204 reward: -248.01393202514052\n",
      "episode: 205 reward: -132.98283609466984\n",
      "episode: 206 reward: -136.93337482553633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 207 reward: -125.15593957762067\n",
      "episode: 208 reward: -253.0425249314722\n",
      "episode: 209 reward: -268.8299869541173\n",
      "episode: 210 reward: -252.8473403617894\n",
      "episode: 211 reward: -132.61120193546267\n",
      "episode: 212 reward: -270.9579964352161\n",
      "episode: 213 reward: -131.34855669213113\n",
      "episode: 214 reward: -131.00768935733026\n",
      "episode: 215 reward: -132.92985278472182\n",
      "episode: 216 reward: -132.84019123412398\n",
      "episode: 217 reward: -124.8756838298895\n",
      "episode: 218 reward: -129.8676487092138\n",
      "episode: 219 reward: -133.33154871056428\n",
      "episode: 220 reward: -130.67092641961224\n",
      "episode: 221 reward: -298.8569392495992\n",
      "episode: 222 reward: -445.42861013695665\n",
      "episode: 223 reward: -259.60402354894927\n",
      "episode: 224 reward: -6.99545469709659\n",
      "episode: 225 reward: -133.2262897376931\n",
      "episode: 226 reward: -256.996547530838\n",
      "episode: 227 reward: -129.4792150247529\n",
      "episode: 228 reward: -642.5008652630114\n",
      "episode: 229 reward: -133.4147409761136\n",
      "episode: 230 reward: -418.2579560907369\n",
      "episode: 231 reward: -368.95899572685624\n",
      "episode: 232 reward: -135.37886480347848\n",
      "episode: 233 reward: -137.78018566001128\n",
      "episode: 234 reward: -136.24598583637245\n",
      "episode: 235 reward: -252.40689047908927\n",
      "episode: 236 reward: -128.56323518600246\n",
      "episode: 237 reward: -255.93197196545435\n",
      "episode: 238 reward: -264.6339987488561\n",
      "episode: 239 reward: -136.62418232629307\n",
      "episode: 240 reward: -467.3669173834141\n",
      "episode: 241 reward: -129.4125654195733\n",
      "episode: 242 reward: -261.8547395627975\n",
      "episode: 243 reward: -263.35054858824293\n",
      "episode: 244 reward: -132.6535565763755\n",
      "episode: 245 reward: -7.169187671476193\n",
      "episode: 246 reward: -130.00264630134333\n",
      "episode: 247 reward: -399.81156659213286\n",
      "episode: 248 reward: -136.09926745444713\n",
      "episode: 249 reward: -380.1732876855889\n",
      "\n",
      "(50000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([state, action]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1179, -0.1179,  0.1210, -0.2299,  0.1210, -0.1179,  0.5470,  0.5470,\n",
       "          0.1210,  0.5154, -0.2299,  0.1210,  0.5154,  0.5470, -0.2299, -0.1179],\n",
       "        [ 0.3900,  0.3900, -0.8305,  0.2822, -0.8305,  0.3900,  1.8698,  1.8698,\n",
       "         -0.8305, -1.0892,  0.2822, -0.8305, -1.0892,  1.8698,  0.2822,  0.3900],\n",
       "        [-0.2651, -0.2651,  0.2852, -0.5086,  0.2852, -0.2651,  0.1309,  0.1309,\n",
       "          0.2852, -0.5207, -0.5086,  0.2852, -0.5207,  0.1309, -0.5086, -0.2651],\n",
       "        [-0.1693, -0.1693, -1.5956,  1.3470, -1.5956, -0.1693, -0.3691, -0.3691,\n",
       "         -1.5956, -0.9737,  1.3470, -1.5956, -0.9737, -0.3691,  1.3470, -0.1693],\n",
       "        [ 0.5413,  0.5413,  0.2120,  0.5419,  0.2120,  0.5413, -0.8155, -0.8155,\n",
       "          0.2120, -1.7334,  0.5419,  0.2120, -1.7334, -0.8155,  0.5419,  0.5413],\n",
       "        [ 1.1363,  1.1363, -2.1591, -0.2004, -2.1591,  1.1363,  1.1208,  1.1208,\n",
       "         -2.1591,  0.4594, -0.2004, -2.1591,  0.4594,  1.1208, -0.2004,  1.1363],\n",
       "        [-0.7458, -0.7458, -0.5462,  0.5386, -0.5462, -0.7458, -0.5862, -0.5862,\n",
       "         -0.5462,  0.8978,  0.5386, -0.5462,  0.8978, -0.5862,  0.5386, -0.7458],\n",
       "        [ 0.0364,  0.0364, -0.3865,  1.4766, -0.3865,  0.0364,  1.5736,  1.5736,\n",
       "         -0.3865,  2.4900,  1.4766, -0.3865,  2.4900,  1.5736,  1.4766,  0.0364],\n",
       "        [-2.5262, -2.5262, -0.7292, -0.5541, -0.7292, -2.5262, -0.9876, -0.9876,\n",
       "         -0.7292,  0.3124, -0.5541, -0.7292,  0.3124, -0.9876, -0.5541, -2.5262],\n",
       "        [ 0.2910,  0.2910, -0.1880, -0.3917, -0.1880,  0.2910,  1.0153,  1.0153,\n",
       "         -0.1880,  1.4753, -0.3917, -0.1880,  1.4753,  1.0153, -0.3917,  0.2910],\n",
       "        [ 0.2826,  0.2826,  1.6023, -1.1571,  1.6023,  0.2826,  0.6088,  0.6088,\n",
       "          1.6023, -0.4388, -1.1571,  1.6023, -0.4388,  0.6088, -1.1571,  0.2826],\n",
       "        [-0.9620, -0.9620,  0.0685,  1.3067,  0.0685, -0.9620, -1.0938, -1.0938,\n",
       "          0.0685,  0.0151,  1.3067,  0.0685,  0.0151, -1.0938,  1.3067, -0.9620],\n",
       "        [-0.7935, -0.7935, -0.2503, -2.1670, -0.2503, -0.7935, -0.0135, -0.0135,\n",
       "         -0.2503, -0.3964, -2.1670, -0.2503, -0.3964, -0.0135, -2.1670, -0.7935],\n",
       "        [ 0.6217,  0.6217,  0.9042, -0.7605,  0.9042,  0.6217, -1.0307, -1.0307,\n",
       "          0.9042, -1.5601, -0.7605,  0.9042, -1.5601, -1.0307, -0.7605,  0.6217],\n",
       "        [ 1.0127,  1.0127, -0.5581, -0.5293, -0.5581,  1.0127, -0.9959, -0.9959,\n",
       "         -0.5581, -0.4113, -0.5293, -0.5581, -0.4113, -0.9959, -0.5293,  1.0127],\n",
       "        [-0.2080, -0.2080,  1.0597,  1.7151,  1.0597, -0.2080, -1.2259, -1.2259,\n",
       "          1.0597,  0.1827,  1.7151,  1.0597,  0.1827, -1.2259,  1.7151, -0.2080]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi[action]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
