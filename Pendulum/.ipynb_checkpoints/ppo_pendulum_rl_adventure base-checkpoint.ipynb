{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#관전포인트 : 경험모을 때 gpu 를 사용할 수 있는가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda:1\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "#         self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage): # 전체 데이터 셋에서 mini_batch 를 만드는 것이다.\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2): # training\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 32\n",
    "lr               = 1e-3\n",
    "num_steps        = 128\n",
    "mini_batch_size  = 256\n",
    "ppo_epochs       = 30\n",
    "threshold_reward = -100\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 15000\n",
    "frame_idx  = 0\n",
    "test_rewards = []\n",
    "early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "while frame_idx < max_frames and not early_stop: #15000 epoch 반복\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps): #경험 모으기 - gpu 쓰는구나 . 하지만 여전히 DataParallel 들어갈 여지는 없어보인다. \n",
    "        #-> 아.. state 가 벡터 1개가 아닐 것 같다.. 16개네. gpu 쓸만하다. DataParallel 도 가능할듯?\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy()) #done 한다고 끝내질 않네??\n",
    "        \n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 1000 == 0: # 1000번 마다 plot 그려주기\n",
    "            test_reward = np.mean([test_env() for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "            \n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device) #경험 1개 모은거로 학습하기\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving trajectories for GAIL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -129.13799296987702\n",
      "episode: 1 reward: -932.7678497310228\n",
      "episode: 2 reward: -898.3875399316933\n",
      "episode: 3 reward: -2.808151698071401\n",
      "episode: 4 reward: -125.45047444486762\n",
      "episode: 5 reward: -132.9797918922758\n",
      "episode: 6 reward: -355.55715321522564\n",
      "episode: 7 reward: -126.53399458791888\n",
      "episode: 8 reward: -304.5633335029137\n",
      "episode: 9 reward: -129.98196937008424\n",
      "episode: 10 reward: -994.7605653622121\n",
      "episode: 11 reward: -930.2061279892231\n",
      "episode: 12 reward: -932.7953863528325\n",
      "episode: 13 reward: -133.34607957575813\n",
      "episode: 14 reward: -2.6756637893193287\n",
      "episode: 15 reward: -1063.9012682520083\n",
      "episode: 16 reward: -266.8251385437975\n",
      "episode: 17 reward: -926.6840231899974\n",
      "episode: 18 reward: -132.18033146144106\n",
      "episode: 19 reward: -931.756534067114\n",
      "episode: 20 reward: -131.52278813159333\n",
      "episode: 21 reward: -309.7484743180258\n",
      "episode: 22 reward: -270.0331669426037\n",
      "episode: 23 reward: -138.33266881996923\n",
      "episode: 24 reward: -257.2107103810072\n",
      "episode: 25 reward: -258.97016047345096\n",
      "episode: 26 reward: -132.21712519539943\n",
      "episode: 27 reward: -952.6323214769114\n",
      "episode: 28 reward: -923.398953692726\n",
      "episode: 29 reward: -266.53529120203046\n",
      "episode: 30 reward: -308.2910077855089\n",
      "episode: 31 reward: -924.4489686540111\n",
      "episode: 32 reward: -124.46810973871439\n",
      "episode: 33 reward: -251.89970391702104\n",
      "episode: 34 reward: -125.78640367267302\n",
      "episode: 35 reward: -128.38773450226194\n",
      "episode: 36 reward: -270.87304895809007\n",
      "episode: 37 reward: -912.235413247864\n",
      "episode: 38 reward: -0.8944808540191336\n",
      "episode: 39 reward: -128.11820641992387\n",
      "episode: 40 reward: -927.5716425721282\n",
      "episode: 41 reward: -1.0423086308158611\n",
      "episode: 42 reward: -914.9669433218357\n",
      "episode: 43 reward: -129.31758559660167\n",
      "episode: 44 reward: -1104.7327196072579\n",
      "episode: 45 reward: -125.39485458242739\n",
      "episode: 46 reward: -2.743158140715114\n",
      "episode: 47 reward: -129.89923707137328\n",
      "episode: 48 reward: -253.29301169185362\n",
      "episode: 49 reward: -132.86592432159625\n",
      "episode: 50 reward: -250.63927686574945\n",
      "episode: 51 reward: -935.6076038310291\n",
      "episode: 52 reward: -128.60649489348634\n",
      "episode: 53 reward: -137.55473297770786\n",
      "episode: 54 reward: -274.1265701005347\n",
      "episode: 55 reward: -133.61699829156078\n",
      "episode: 56 reward: -940.3390930669398\n",
      "episode: 57 reward: -930.1068982786504\n",
      "episode: 58 reward: -373.8544619090089\n",
      "episode: 59 reward: -133.50348804954217\n",
      "episode: 60 reward: -359.1841572229469\n",
      "episode: 61 reward: -920.7562389255538\n",
      "episode: 62 reward: -249.75968924160412\n",
      "episode: 63 reward: -126.42306840555447\n",
      "episode: 64 reward: -277.75618817717844\n",
      "episode: 65 reward: -1.3875691391829241\n",
      "episode: 66 reward: -128.5679045164196\n",
      "episode: 67 reward: -137.79521467094776\n",
      "episode: 68 reward: -936.6836964766478\n",
      "episode: 69 reward: -131.29179730647724\n",
      "episode: 70 reward: -133.79635374394735\n",
      "episode: 71 reward: -2.5693682969136367\n",
      "episode: 72 reward: -130.8166680155214\n",
      "episode: 73 reward: -907.5935183936049\n",
      "episode: 74 reward: -930.0332876860122\n",
      "episode: 75 reward: -961.8582146402237\n",
      "episode: 76 reward: -133.6661891020748\n",
      "episode: 77 reward: -133.8429714477416\n",
      "episode: 78 reward: -129.27940121254102\n",
      "episode: 79 reward: -129.9189973370132\n",
      "episode: 80 reward: -135.04508279492794\n",
      "episode: 81 reward: -877.3518487280624\n",
      "episode: 82 reward: -366.59514840773744\n",
      "episode: 83 reward: -250.03888352541907\n",
      "episode: 84 reward: -125.95179230042102\n",
      "episode: 85 reward: -932.8266574701507\n",
      "episode: 86 reward: -130.48601507011864\n",
      "episode: 87 reward: -1.2911448004708521\n",
      "episode: 88 reward: -120.900207739772\n",
      "episode: 89 reward: -3.5618921440220306\n",
      "episode: 90 reward: -941.0308969760575\n",
      "episode: 91 reward: -123.23860123131382\n",
      "episode: 92 reward: -1.6927558413366004\n",
      "episode: 93 reward: -134.0225461020283\n",
      "episode: 94 reward: -264.0299663240528\n",
      "episode: 95 reward: -907.6570532009323\n",
      "episode: 96 reward: -300.28768732163513\n",
      "episode: 97 reward: -133.18720240115115\n",
      "episode: 98 reward: -938.657305303873\n",
      "episode: 99 reward: -256.11002628912604\n",
      "episode: 100 reward: -277.41283585937117\n",
      "episode: 101 reward: -134.24144808734238\n",
      "episode: 102 reward: -907.728710633999\n",
      "episode: 103 reward: -1.1169185107961506\n",
      "episode: 104 reward: -1.9544039856833073\n",
      "episode: 105 reward: -132.92198126279231\n",
      "episode: 106 reward: -253.83576016831123\n",
      "episode: 107 reward: -260.2119534024388\n",
      "episode: 108 reward: -945.4788275209733\n",
      "episode: 109 reward: -919.190926951718\n",
      "episode: 110 reward: -262.5379750878487\n",
      "episode: 111 reward: -120.79352946505404\n",
      "episode: 112 reward: -936.6287358485389\n",
      "episode: 113 reward: -130.27403765746948\n",
      "episode: 114 reward: -121.11684451068038\n",
      "episode: 115 reward: -2.1676182235367545\n",
      "episode: 116 reward: -130.73004189531505\n",
      "episode: 117 reward: -1023.0735766386474\n",
      "episode: 118 reward: -133.1199497088308\n",
      "episode: 119 reward: -964.3899257257615\n",
      "episode: 120 reward: -132.73247407761434\n",
      "episode: 121 reward: -2.1272566419059866\n",
      "episode: 122 reward: -929.8137505162185\n",
      "episode: 123 reward: -130.39954438215682\n",
      "episode: 124 reward: -0.8494186654475263\n",
      "episode: 125 reward: -289.58121368674796\n",
      "episode: 126 reward: -140.93114686899486\n",
      "episode: 127 reward: -269.1025634527143\n",
      "episode: 128 reward: -130.26252070327553\n",
      "episode: 129 reward: -979.8350512019905\n",
      "episode: 130 reward: -134.65989691287334\n",
      "episode: 131 reward: -134.92061028299895\n",
      "episode: 132 reward: -2.2408082144541064\n",
      "episode: 133 reward: -809.2320139800148\n",
      "episode: 134 reward: -256.5598593678237\n",
      "episode: 135 reward: -244.2275543146463\n",
      "episode: 136 reward: -1.783660427225332\n",
      "episode: 137 reward: -130.44579602912378\n",
      "episode: 138 reward: -943.6770657133956\n",
      "episode: 139 reward: -120.35942585738543\n",
      "episode: 140 reward: -0.9530806781163702\n",
      "episode: 141 reward: -258.77985521228544\n",
      "episode: 142 reward: -260.4852101029978\n",
      "episode: 143 reward: -263.84630944490686\n",
      "episode: 144 reward: -1.1118431123639814\n",
      "episode: 145 reward: -132.4081260273409\n",
      "episode: 146 reward: -263.3559832450617\n",
      "episode: 147 reward: -266.27531809857925\n",
      "episode: 148 reward: -405.10284539853495\n",
      "episode: 149 reward: -945.0159511289284\n",
      "episode: 150 reward: -132.5192999335082\n",
      "episode: 151 reward: -132.52473951155812\n",
      "episode: 152 reward: -972.0508462842367\n",
      "episode: 153 reward: -129.8827605525468\n",
      "episode: 154 reward: -1.749245883188411\n",
      "episode: 155 reward: -940.1392750685176\n",
      "episode: 156 reward: -129.82365538111577\n",
      "episode: 157 reward: -132.11820194977216\n",
      "episode: 158 reward: -140.28836566169122\n",
      "episode: 159 reward: -248.90070393945433\n",
      "episode: 160 reward: -977.673622888682\n",
      "episode: 161 reward: -267.53644507133123\n",
      "episode: 162 reward: -116.02720996191292\n",
      "episode: 163 reward: -263.5666981444745\n",
      "episode: 164 reward: -289.17310659820424\n",
      "episode: 165 reward: -266.7639923955883\n",
      "episode: 166 reward: -261.91771726488355\n",
      "episode: 167 reward: -134.35125739956445\n",
      "episode: 168 reward: -362.51179263316817\n",
      "episode: 169 reward: -261.69303782316854\n",
      "episode: 170 reward: -3.640767935774865\n",
      "episode: 171 reward: -122.06751120475116\n",
      "episode: 172 reward: -131.07983284801028\n",
      "episode: 173 reward: -918.3289860475116\n",
      "episode: 174 reward: -135.32980173334042\n",
      "episode: 175 reward: -1.0060693121967579\n",
      "episode: 176 reward: -269.71008378210666\n",
      "episode: 177 reward: -917.0022582653617\n",
      "episode: 178 reward: -1044.1265380263324\n",
      "episode: 179 reward: -924.4669998883431\n",
      "episode: 180 reward: -129.5335634839728\n",
      "episode: 181 reward: -1009.6412348231681\n",
      "episode: 182 reward: -933.6327218825838\n",
      "episode: 183 reward: -130.2699885413551\n",
      "episode: 184 reward: -965.3323413446508\n",
      "episode: 185 reward: -242.48770812103945\n",
      "episode: 186 reward: -1032.5077471543348\n",
      "episode: 187 reward: -133.89702065403205\n",
      "episode: 188 reward: -2.489535955399348\n",
      "episode: 189 reward: -1.3883804353221265\n",
      "episode: 190 reward: -937.9500533108752\n",
      "episode: 191 reward: -267.7054129569532\n",
      "episode: 192 reward: -251.22958049467076\n",
      "episode: 193 reward: -132.52943828685042\n",
      "episode: 194 reward: -960.5930859162173\n",
      "episode: 195 reward: -126.27208804272945\n",
      "episode: 196 reward: -123.87883752059219\n",
      "episode: 197 reward: -926.9745546111114\n",
      "episode: 198 reward: -135.22316318043585\n",
      "episode: 199 reward: -917.3022092780443\n",
      "episode: 200 reward: -133.65593421902557\n",
      "episode: 201 reward: -972.8495018046153\n",
      "episode: 202 reward: -134.59481703046941\n",
      "episode: 203 reward: -378.48147388241597\n",
      "episode: 204 reward: -129.7931960870596\n",
      "episode: 205 reward: -135.28726966609463\n",
      "episode: 206 reward: -131.7948652529053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 207 reward: -122.14841712884714\n",
      "episode: 208 reward: -804.3040521270282\n",
      "episode: 209 reward: -132.79371249983942\n",
      "episode: 210 reward: -1042.591844301673\n",
      "episode: 211 reward: -917.3607124071959\n",
      "episode: 212 reward: -451.63520892968063\n",
      "episode: 213 reward: -1.8774766727918606\n",
      "episode: 214 reward: -936.6550226402499\n",
      "episode: 215 reward: -253.01069072373878\n",
      "episode: 216 reward: -3.058951246148162\n",
      "episode: 217 reward: -1.2593094238418208\n",
      "episode: 218 reward: -923.2652456991628\n",
      "episode: 219 reward: -131.15464319324207\n",
      "episode: 220 reward: -945.0847178129874\n",
      "episode: 221 reward: -4.98523319824422\n",
      "episode: 222 reward: -906.0704037191306\n",
      "episode: 223 reward: -1014.3151007886493\n",
      "episode: 224 reward: -132.5096131671335\n",
      "episode: 225 reward: -1011.2131259630145\n",
      "episode: 226 reward: -243.6291184878994\n",
      "episode: 227 reward: -975.7846838289948\n",
      "episode: 228 reward: -255.61312818052983\n",
      "episode: 229 reward: -131.3554855582014\n",
      "episode: 230 reward: -132.6289362315288\n",
      "episode: 231 reward: -941.3589644232342\n",
      "episode: 232 reward: -129.94270741006318\n",
      "episode: 233 reward: -930.0526191652615\n",
      "episode: 234 reward: -123.32769642950927\n",
      "episode: 235 reward: -427.3270990260053\n",
      "episode: 236 reward: -127.59079707691278\n",
      "episode: 237 reward: -251.84060780082365\n",
      "episode: 238 reward: -123.14892295397277\n",
      "episode: 239 reward: -0.991227118397129\n",
      "episode: 240 reward: -896.7651584844908\n",
      "episode: 241 reward: -2.170238219442166\n",
      "episode: 242 reward: -134.2077519579944\n",
      "episode: 243 reward: -256.73724770645003\n",
      "episode: 244 reward: -282.6191082240198\n",
      "episode: 245 reward: -897.6742365949115\n",
      "episode: 246 reward: -926.3272308933736\n",
      "episode: 247 reward: -131.34142883101458\n",
      "episode: 248 reward: -120.6242464940242\n",
      "episode: 249 reward: -132.00964656389033\n",
      "\n",
      "(50000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([state, action]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
